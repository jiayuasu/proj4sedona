name: Benchmarks

on:
  push:
    branches: [main, master]
    paths:
      - 'src/**'
      - 'scripts/**'
      - 'pom.xml'
      - '.github/workflows/benchmarks.yml'
  pull_request:
    branches: [main, master]
    paths:
      - 'src/**'
      - 'scripts/**'
      - 'pom.xml'
      - '.github/workflows/benchmarks.yml'
  workflow_dispatch:

env:
  JAVA_VERSION: '17'
  PYTHON_VERSION: '3.11'

jobs:
  benchmarks:
    name: Correctness & Speed Benchmarks
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Java ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: maven

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: scripts/pyproj-reference/requirements.txt

      - name: Install Python dependencies
        run: |
          pip install -r scripts/pyproj-reference/requirements.txt

      - name: Run unit tests
        run: mvn test -B

      - name: Run correctness and speed benchmarks
        run: mvn verify -Pbenchmarks -B

      # Run pyproj benchmarks for comparison
      - name: Run pyproj benchmarks
        run: |
          python scripts/pyproj-reference/run_pyproj_benchmarks.py \
            --output target/pyproj_benchmark_results.json

      # Generate comparison report
      - name: Generate benchmark comparison report
        run: |
          python scripts/pyproj-reference/compare_benchmarks.py \
            --pyproj target/pyproj_benchmark_results.json \
            --java target/java_benchmark_results.json \
            --output target/benchmark_comparison_report.md || true

      # Post test summary to GitHub Actions
      - name: Post summary to GitHub Actions
        if: always()
        run: |
          echo "## ðŸ§ª Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Extract test results from failsafe reports
          if [ -d "target/failsafe-reports" ]; then
            echo "### Correctness Benchmarks" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| Test Suite | Tests | Passed | Failed | Skipped |" >> $GITHUB_STEP_SUMMARY
            echo "|------------|-------|--------|--------|---------|" >> $GITHUB_STEP_SUMMARY
            
            for file in target/failsafe-reports/*.txt; do
              if [ -f "$file" ]; then
                suite=$(basename "$file" .txt)
                line=$(grep "^Tests run:" "$file" 2>/dev/null || echo "")
                if [ -n "$line" ]; then
                  tests=$(echo "$line" | sed 's/.*Tests run: \([0-9]*\).*/\1/')
                  failures=$(echo "$line" | sed 's/.*Failures: \([0-9]*\).*/\1/')
                  errors=$(echo "$line" | sed 's/.*Errors: \([0-9]*\).*/\1/')
                  skipped=$(echo "$line" | sed 's/.*Skipped: \([0-9]*\).*/\1/')
                  passed=$((tests - failures - errors - skipped))
                  
                  if [ "$failures" -eq 0 ] && [ "$errors" -eq 0 ]; then
                    status="âœ…"
                  else
                    status="âŒ"
                  fi
                  
                  echo "| $status $suite | $tests | $passed | $((failures + errors)) | $skipped |" >> $GITHUB_STEP_SUMMARY
                fi
              fi
            done
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Add benchmark comparison report
          if [ -f "target/benchmark_comparison_report.md" ]; then
            echo "---" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## ðŸš€ Speed Benchmarks" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            cat target/benchmark_comparison_report.md >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload test reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-reports
          path: |
            target/surefire-reports/
            target/failsafe-reports/
          retention-days: 7

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: |
            target/pyproj_benchmark_results.json
            target/java_benchmark_results.json
            target/benchmark_comparison_report.md
          retention-days: 30

